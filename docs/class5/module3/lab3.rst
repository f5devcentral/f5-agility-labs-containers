Section 3: Container Connector in Action
========================================

This section of the lab will cover creating OpenShift resources that the F5 Container Connector will process and use to update the BIG-IP configuration and leverages the work you did in the previous sections.


Operational Overview
---------------------
The Container Connector watches for events being generated by the Openshift API server and takes action when it sees an OpenShift ConfigMap or Route resource that has an F5-specific label defined.  The Container Connector parses the ConfigMap or Route resource
and updates the BIG-IP configuration to match the desired state as defined by those resources.

In addition to watching and responding to events in realtime, the Container Connector periodically queries the OpenShift API for the current status and updates the BIG-IP as needed.  This interval is 30 seconds by default but is a startup value that can be modified.

An instance of the Container Connector can watch for changes in all namespaces (projects), a single namespace or a discrete list of namespaces.  Additionally, an instance of the Container Connector is configured to make configuration changes in a single non-common BIG-IP partition.

OpenShift runs on top of Kubernetes and the same Container Connector works for both, but many of the Container Connector features apply to both while some apply only to OpenShift, like Routes, while others, like Ingress, apply only to Kubernetes.

You can find detailed information about configuring, deploying and using the F5 Container Connector as well as configuration options for ConfigMaps and Routes
https://clouddocs.f5.com/containers/v2/#

Additionally, you can get more detailed information about an OpenShift command by using **oc <command> -help**.  So, for example, if you wanted to find out more about the **oc create** command, you would do the following:


.. code-block:: console

    [root@ose-mstr01 garrison]# oc create -help




In the following exercises, we will create the following OpenShift resource types:

* ConfigMaps
* Routes

Additionally, we will also create variations of each resource type.

.. NOTE::

    You will use the same Windows jumphost as you used in the previous sections to complete the exercises in this section.

    Also, all the resource definition yaml files have been pre-created and can be found on the ose-master server under /root/agility2018/apps



Excercise #1: ConfigMap - Basic
-------------------------------

An OpenShift ConfigMap is one of the resource types that the F5 Container Connector watches for.    The Container Connector will read the ConfigMap
and create a virtual server, node(s), a pool, pool member(s) and a pool health monitor.

In this exercise, we will create a ConfigMap that defines the objects that the Container Connector should configure on the BIG-IP.

To complete this exercise, we will perform the following steps:

* Step 1: Deploy demo application
* Step 2: Create a service to expose the demo application
* Step 3: Create a ConfigMap that declares desired BIG-IP configuration
* Step 4: Review the BIG-IP configuration
* Step 5: Test the application
* Step 6: Scale the application
* Step 7: Test the scaled application


**Step 1**: Deploy demo application

From ose-master, review the following deployment -> demo-app.yaml

.. code-block:: console

    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
    name: f5demo
    spec:
    replicas: 1
    template:
        metadata:
        labels:
            app: f5demo
            tier: frontend
        spec:
        containers:
        - name: f5demo
            image: kmunson1973/f5demo:1.0.0
            ports:
            - containerPort: 8080


**Step 2:** Create service to expose application

In order for an application to be accessible outside of the OpenShift cluster, a service must be created.  The service uses a label selector to reference the application to be exposed.
Additionally, the service also specifies the container port that the application is listening on.

From ose-master, review the following deployment: demo-app-service.yaml

.. code-block:: console

    apiVersion: v1
    kind: Service
    metadata:
    name: f5demo
    labels:
        app: f5demo
        tier: frontend
    spec:
    ports:
    - port: 8080
    selector:
        app: f5demo
        tier: frontend

Now that we have reviewed the Service, we need to actually create the Service by deploying it to OpenShift by using the **oc create** command:

.. code-block:: console

    [root@ose-mstr01 garrison]# oc create -f app-route.yaml
    route "front-end-route" created



**Step 3:** Create ConfigMap

A ConfigMap is used to define the BIG-IP objects that need to be created to enable access to the application via the BIG-IP.
The ConfigMap below defines a virtual server (frontend), pool (backend) and health monitor (healthMonitor).  Additionally, the ConfigMap references the service
created above in step #2.

The label, **f5type: virtual-server**, in the ConfigMap definition is what triggers the F5 Container Connector to process this ConfigMap.

A **ConfigMap** points to a **Service** which points to a **Pod(s)** where the application is running.

From ose-master, review the ConfigMap resource app-configmap.yaml

.. code-block:: console

    kind: ConfigMap
    apiVersion: v1
    metadata:
    # name of the resource to create on the BIG-IP
    name: myfronted-http.vs
    # The namespace to create the object in.
    # The k8s-bigip-ctlr watches all namespaces by default (as of v1.1).
    # If the k8s-bigip-ctlr is watching a specific namespace(s),
    # this setting must match the namespace of the Service you want to proxy
    # -AND- the namespace(s) the k8s-bigip-ctlr watches.
    namespace: f5demo
    labels:
        # tells the k8s-bigip-ctlr to watch this ConfigMap
        f5type: virtual-server
    data:
    # NOTE: schema v0.1.4 is required as of k8s-bigip-ctlr v1.3.0
    schema: "f5schemadb://bigip-virtual-server_v0.1.7.json"
    data: |
        {
        "virtualServer": {
            "backend": {
            "servicePort": 8080,
            "serviceName": "f5demo",
            "healthMonitors": [{
                "interval": 5,
                "protocol": "http",
                "send": "GET /\r\n",
                "timeout": 16
            }]
            },
            "frontend": {
            "virtualAddress": {
                "port": 80,
                "bindAddr": "10.10.202.200"
            },
            "partition": "ocp",
            "balance": "least-connections-node",
            "mode": "http"
            }
        }
        }



Now that we have reviewed the ConfigMap, we need to actually create the ConfigMap by deploying it to OpenShift by using the **oc create** command:

.. code-block:: console

    [root@ose-mstr01 garrison]# oc create -f app-configmap.yaml
    configmap "f5demo.vs" created



**Step 4:** Review BIG-IP configuration

TODO


**Step 5:** Test the application

In this step, you will use a browser to access the application you previously deployed to OpenShift.

Using the Chrome browser, navigate to http://x.x.x.x to access the application.  On the application web page, the **Server IP** and **Server Port** are the IP address and port of the pod where the application is running. 



**Step 6:** Scale the application

The application deployed in step #1 is a single replica (instance).  In this exercise, we are going to increase the number of replicas and then check the BIG-IP configuration to see what's changed.

When the deployment replica count is scaled up or scaled down, an OpenShift event is generated and the Container Connector sees the event and adds or removes pool members as appropriate.

To scale the number of replicas, we will use the OpenShift **oc scale** command.  We will be scaling the demo app deployment and so we first need to get the name of the deployment.

From ose-master, issue the following command:

.. code-block:: console

    [root@ose-mstr01 ~]# oc get deployment
    NAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
    my-frontend   1         1         1            1           2m


You can see from the output that the deployment is named **my-frontend** and we will use that name for the next command.

From the ose-master host, entering the following command to set the replica count for the deployment to 5 instances:

.. code-block:: console

    [root@ose-mstr01 ~]# oc scale --replicas=5 deployment/my-frontend
    deployment "my-frontend" scaled


Now go examine the BIG-IP pool members...

TODO


**Step 7:** Test the scaled application

In this step, you will use a browser to access the application you scaled up.

Using the Chrome browser, navigate to http://x.x.x.x to access the application.  On the application web page, the **Server IP** and **Server Port** are the IP address and port of the pod where the application is running. 

If you reload the page every few seconds, you should see the **Server IP** address change.  Because there is more than one instance of the application running, the BIG-IP load balances the application traffic amongst multiple pods.  




TODO


Excercise #2: Route - Basic
---------------------------

An OpenShift Route is one of the resource types that the F5 Container Connector watches for.  A Route defines a hostname or URI mapping to an application.  For example, the hostname "customer.example.com" could map
to the application "customer", hostname "catalog.example.com", might map to the application "catalog", etc.

Similarily, a Route can refer to a URI path so, for example, the URI path "/customer" might map to the application called "customer" and URI path "/catalog",
might map to the application called "catalog".  If a Route only specifies URI paths, the Route applies to all HTTP request hostnames.

Additionally, a Route can refer to both a hostname and a URI path.  So, for example, the 

The F5 Container Connector reads the Route resource and creates a virtual server, node(s), a pool per route path and pool members.  Additionally, the Container Connector
creates a layer 7 BIG-IP traffic policy and associates it with the virtual server.  This layer 7 traffic policy evaluates the hostname or URI path from the request and
forwards the traffic to the pool associated with that path.

A **Route** points to a **Service(s)** which points to a **Pod(s)** where the application is running.

.. NOTE:: 

    All Route resources share two virtual servers:

    * **ose-vserver** for HTTP traffic, and
    * **https-ose-vserver** for HTTPS traffic

    The Container Connector assigns the names shown above by default. To set set custom names, define route-http-vserver and route-https-vserver in the BIG-IP Container Connector Deployment.
    Please see the documentation at: http://clouddocs.f5.com for more details.


To complete this exercise, we will perform the following steps:

* Step 1: Deploy demo application and it's associated Service
* Step 2: Create a Route that defines routing rules based on hostname
* Step 3: Review the BIG-IP configuration

**Step 1:** Deploy demo application and it's associated Service


From ose-master, review the following deployment: app-deployment.yaml

.. code-block:: console

    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
    name: my-frontend
    spec:
    replicas: 1
    template:
        metadata:
        labels:
            app: my-frontend
        spec:
        containers:
        - name: my-frontend
            image: chen23/f5-demo-app:openshift
            ports:
            - containerPort: 8080
            protocol: TCP
    ---
    apiVersion: v1
    kind: Service
    metadata:
    name: my-frontend
    labels:
        name: my-frontend
    namespace: f5demo
    spec:
    type: ClusterIP
    ports:
    - port: 8080
        targetPort: 8080
    selector:
        app: my-frontend


Now that we have reviewed the Deployment, we need to actually create it by deploying it to OpenShift by using the **oc create** command:

.. code-block:: console

    root@ose-mstr01 garrison]# oc create -f app-deployment.yaml
    deployment "my-frontend" created
    service "my-frontend" created



**Step 2:** Create OpenShift Route

From ose-master, review the following deployment: demo-app-route.yaml


.. code-block:: console

    apiVersion: v1
    kind: Route
    metadata:
    labels:
        name: front-end-route
    name: front-end-route
    namespace: f5demo
    annotations:
        # Specify a supported BIG-IP load balancing mode
        virtual-server.f5.com/balance: least-connections-node
        virtual-server.f5.com/health: |
        [
            {
            "path": "mysite.f5demo.com/",
            "send": "HTTP GET /",
            "interval": 5,
            "timeout": 10
            }
        ]
    spec:
    host: mysite.f5demo.com
    path: "/"
    port:
        targetPort: 80
    to:
        kind: Service
        name: front-end


Now that we have reviewed the Route, we need to actually create it by deploying it to OpenShift by using the **oc create** command:

.. code-block:: console

    [root@ose-mstr01 garrison]# oc create -f app-route.yaml
    route "my-frontend-route-unsecured" created


**Step 3:** Review BIG-IP configuration

TODO



Excercise #3: Route - A/B Testing
---------------------------------

The F5 Container Connector supports A/B application testing e.g two different versions of the same application, by using the **weight** parameter of OpenShift Routes.  The **weight** parameter allows you
to establish relative ratios between application "A" and application "B". So, for example, if the first route specifies a weight of 20 and the second a weight of 10,
the application associated with the first route will get twice the number of requests as the application associated with the second route.

Just as in the previous excercise, the F5 Container Connector reads the Route resource and creates a virtual server, node(s), a pool per route path and pool members.  Additionally, the Container Connector
creates a layer 7 BIG-IP traffic policy and associates it with the virtual server.  This layer 7 traffic policy evaluates the hostname or URI path from the request and
forwards the traffic to the pool associated with that path.

However, in order to support A/B testing using OpenShift routes, the Container Connector creates an iRule and a datagroup on the BIG-IP which handles the connection routing based on the assigned weights.

To complete this exercise, we will perform the following steps:

* Step 1: Deploy version 1 and version 2 of demo application and their related Services
* Step 2: Create an OpenShift Route with two paths that defines the weight for each application
* Step 3: Review BIG-IP configuration
* Step 4: Test the application


**Step 1:** Deploy version 1 and version 2 of demo application and their related Services

From ose-master, review the following deployment: app-deployment-ab.yaml

.. code-block:: console

    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
    name: my-frontend-ab-v1
    spec:
    replicas: 1
    template:
        metadata:
        labels:
            app: my-frontend-ab-v1
        spec:
        containers:
        - name: my-frontend-ab-v1
            image: chen23/f5-demo-app:openshift
            ports:
            - containerPort: 8080
            protocol: TCP

    ---
    apiVersion: v1
    kind: Service
    metadata:
    name: my-frontend-ab-v1
    labels:
        name: my-frontend-ab-v1
    namespace: f5demo
    spec:
    type: ClusterIP
    ports:
    - port: 8080
        targetPort: 8080
    selector:
        app: my-frontend-ab-v1

    ---
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
    name: my-frontend-ab-v2
    spec:
    replicas: 1
    template:
        metadata:
        labels:
            app: my-frontend-ab-v2
        spec:
        containers:
        - name: my-frontend-ab-v2
            image: chen23/f5-demo-app:openshift
            ports:
            - containerPort: 8080
            protocol: TCP

    ---
    apiVersion: v1
    kind: Service
    metadata:
    name: my-frontend-ab-v2
    labels:
        name: my-frontend-ab-v2
    namespace: f5demo
    spec:
    type: ClusterIP
    ports:
    - port: 8080
        targetPort: 8080
    selector:
        app: my-frontend-ab-v2


Now that we have reviewed the Deployment, we need to actually create it by deploying it to OpenShift by using the **oc create** command:

.. code-block:: console

    [root@ose-mstr01 garrison]# oc create -f app-deployment-ab-v1.yaml
    deploymentconfig "my-frontend-ab-v1" created
    service "my-frontend-ab-v1" created



**Step 2:** Create OpenShift Route for A/B testing

The basic Route example from the previous excercise only included one path.  In order to support A/B application testing, a Route must be created that has two paths.
In OpenShift, the second path is defined in the **alternateBackends** section of a Route resource.

From ose-master, review the following Route: app-route-ab.yaml

.. code-block:: console

    apiVersion: v1
    kind: Route
    metadata:
    labels:
        name: my-frontend-route-ab
    name: my-frontend-route-ab-unsecured
    namespace: f5demo
    annotations:
        # Specify a supported BIG-IP load balancing mode
        virtual-server.f5.com/balance: least-connections-node
        virtual-server.f5.com/health: |
        [
            {
            "path": "mysite-ab.f5demo.com/",
            "send": "HTTP GET /",
            "interval": 5,
            "timeout": 10
            }
        ]
    spec:
    host: mysite-ab.f5demo.com
    path: "/"
    port:
        targetPort: 8080
    to:
        kind: Service
        name: my-frontend-ab-v1
        weight: 20
    alternateBackends:
    - kind: Service
        name: my-frontend-ab-v2
        weight: 10


Now that we have reviewed the Route, we need to actually create it by deploying it to OpenShift by using the **oc create** command:

.. code-block:: console

    [root@ose-mstr01 garrison]# oc create -f app-route-ab.yaml
    route "my-frontend-route-ab-unsecured" created

Verify that the Route was successfully creating by using the OpenShift **oc get route** command.  Note that, under the "SERVICES" column, the two applications are listed along with their request distribution percentages.

.. code-block:: console

    [root@ose-mstr01 garrison]# oc get route
    NAME                             HOST/PORT              PATH      SERVICES                                        PORT      TERMINATION   WILDCARD
    my-frontend-route-ab-unsecured   mysite-ab.f5demo.com   /         my-frontend-ab-v1(66%),my-frontend-ab-v2(33%)   8080                    None


**Step 4:** Review BIG-IP configuration

TODO


Excercise #4: Route - Attach Existing Virtual
---------------------------------------------

The F5 Container Connector allows you to set a few virtual server configuration elements such as...  If there are virtual server configuration settings that 
you want to set but aren't configurable in an OpenShift Route, the Container Connector supports defining and using an existing virtual server and this allows you to set configuration
elements of the virtual server that the Container Connector doesn't manage without it removing those changes.


To complete this exercise, we will perform the following steps:

* Step 1: Delete the Container Connector deployment instances
* Step 2: Create a virtual server for HTTP traffic and a virtual server for HTTPS traffic and attach metadata
* Step 3: Edit the Container Connector deployment configurations
* Step 4: Restart the Container Connectors


**Step 1:** Delete the Container Connector deployments

In order to create new virtual servers instances and not have them deleted by the Container Connector, we first have to delete the running Container Connector deployments.

First, we need to get the names of the Container Connector deployments.

From the ose-master server, run the following commands:

.. code-block:: console

    [root@ose-mstr01 ~]# oc get deployment -n kube-system
    NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
    bigip01-ctlr   1         1         1            1           4d
    bigip02-ctlr   1         1         1            1           1h


You can see the names (bigip01-ctlr, bigip02-ctlr) in the command output.  Next we will use the **oc delete** command to delete these two deployments.

From the ose-master, run the following command:

.. code-block:: console

    [root@ose-mstr01 ~]# oc delete deployment bigip02-ctlr -n kube-system
    deployment "bigip02-ctlr" deleted

    [root@ose-mstr01 ~]# oc delete deployment bigip02-ctlr -n kube-system
    deployment "bigip02-ctlr" deleted


**Step 2:** Create a virtual server for HTTP traffic and a virtual server for HTTPS traffic and attach metadata

In this step, we will use BIG-IP TMSH commands to create an HTTP virtual server and attach some metadata.  The metadata tells the Container Connector to not remove any configuration setting that are 
not defined by an OpenShift Route resource.

Because the lab uses to BIG-IPs in an HA pair but without config autosync enabled, the TMSH commands must be run on each BIG-IP (or should we issue sync command?)

Connect to BIG-IP01 and BIG-IP02 via SSH using the mRemoteNG application and issue the following commands on each BIG-IP:

.. code-block:: console

    tmsh create ltm virtual my-ose-vserver destination "10.10.201.240:80" ip-protocol "6" profiles add { http } metadata add { cccl-whitelist { value 1 }}

    tmsh create ltm virtual my-ose-https-vserver destination "10.10.201.240:443" ip-protocol "6" profiles add { http {} clientssl {context clientside}} metadata add { cccl-whitelist { value 1 }}


**Step 3:** Edit the Container Connector deployment configurations

.. NOTE::

    When using OpenShift Routes, the Container Connector supports creating one BIG-IP HTTP and one HTTPS virtual server.  When using OpenShift Configmaps,
    there is a 1:1 relationship between a ConfigMap and BIG-IP virtual servers e.g. a BIG-IP virtual server will be created for each ConfigMap that has an F5 label defined.


In addition to the whitelist metadata that was added when the two virtual servers were created in the previous step, the Container Connector deployment configuration 
must be updated with the names of those two virtual servers.

From ose-master, use vi to edit the two Container Connector deployment configuration files one at a time:

.. code-block:: console

    [root@ose-mstr01 ocp]# vi bigip01-cc.yaml

    [root@ose-mstr01 ocp]# vi bigip02-cc.yaml

In **each** Container Connector deployment configuration file, update the following arguments:

original: --route-http-vserver=ocp-vserver

updated:  --route-http-vserver=my-ocp-vserver

original: --route-https-vserver=ocp-https-vserver
 
updated: --route-https-vserver=-my-ocp-https-vserver

**Step 4:** Restart the Container Connectors

In this step, we will restart the two Containers Connector with the updated arguments.

From ose-master, run the following commands:

.. code-block:: console

    [root@ose-mstr01 ocp]# oc create -f bigip01-cc.yaml
    deployment "bigip01-ctlr" created

    [root@ose-mstr01 ocp]# oc create -f bigip02-cc.yaml
    deployment "bigip02-ctlr" created


TODO


Excercise #5: Container Connector Troubleshooting
-------------------------------------------------

